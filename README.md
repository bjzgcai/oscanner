# Engineer Capability Assessment System

AI-powered system for evaluating engineering capabilities by analyzing GitHub and Gitee commits, code changes, and collaboration patterns using a six-dimensional evaluation framework.

## Overview

This system uses LLM-powered analysis (Claude 4.5 Haiku) to evaluate software engineers based on actual code commits, diffs, and collaboration patterns from GitHub and Gitee repositories. Features a modern React + Antd dashboard and FastAPI backend with intelligent caching.

## Usage

### Quick Start: Full Context + Cached Evaluation (Best Approach! â­)

Evaluate contributors one at a time using **full repository context** with **caching**:

```bash
# Evaluate all contributors with caching
python full_context_cached_evaluator.py

# Or evaluate specific contributor
python full_context_cached_evaluator.py "contributor_name"

# Force re-evaluation (ignore cache)
python full_context_cached_evaluator.py "contributor_name" --force
```

**Why This is Best:**
- âœ… **Full repo context** (~650k tokens) for accuracy
- âœ… **Evaluate ONE contributor at a time** for flexibility
- âœ… **Caches each result** to avoid re-evaluation
- âœ… **Add new contributors later** without re-evaluating everyone
- ğŸ“Š ~100-200k tokens per contributor (first time)
- ğŸ’° ~$0.005-0.01 per contributor (first time)
- ğŸ†“ **FREE** when using cache!

See [BEST_APPROACH.md](BEST_APPROACH.md) for full details.

---

### Alternative: Full Repo Analysis (All at Once)

Evaluate ALL contributors in ONE API call:

```bash
python full_repo_evaluator.py
```

- Uses ~650k tokens, costs ~$0.03
- Evaluates all contributors at once
- No caching (re-evaluates everything each time)

See [FULL_REPO_ANALYSIS_EXPLAINED.md](FULL_REPO_ANALYSIS_EXPLAINED.md) for details.

---

### Alternative: Server + Dashboard

For API-based evaluation with caching:

#### 1. Configure Ports (Optional)

Edit `.env.local` files to configure ports (defaults: evaluator=8000, webapp=3000):

```bash
# evaluator/.env.local
PORT=8000

# webapp/.env.local
PORT=3000
```

#### 2. Start the Services

**Development Mode** (with auto-reload):
```bash
./start_dev.sh
```

**Production Mode** (optimized):
```bash
./start_production.sh
```

**Evaluator Only**:
```bash
./start_server.sh
```

#### 3. Access the Application

- **Webapp Dashboard**: http://localhost:3000 (or your configured PORT)
- **API Server**: http://localhost:8000 (or your configured PORT)
- **API Documentation**: http://localhost:8000/docs

#### 4. Analyze Engineers

**Via Dashboard:**
1. Enter a GitHub/Gitee repository URL
2. Click "Analyze Repository"
3. Select a contributor to evaluate
4. View AI-powered evaluation results with scores and charts

**Via API:**

```bash
# Evaluate a contributor (full analysis of all commits)
curl -X POST "http://localhost:8000/api/evaluate/octocat/Hello-World/octocat"
```

**Moderate Mode (Diffs + Files):**

```bash
# Per-contributor evaluation with file context
python example_moderate_evaluation.py
```

---

## Evaluation Approaches Comparison

| Approach | Tokens | API Calls | Cost | Caching | Best For |
|----------|--------|-----------|------|---------|----------|
| **Full Context + Cached** â­ | ~100-200k each | 1 per contributor | $0.005-0.01 each | âœ… Yes | **Complete + Flexible** |
| **Full Repo Analysis** | ~650k | 1 | $0.03 | âŒ No | One-time team evaluation |
| **Moderate per-contributor** | ~45k each | N | $0.005Ã—N | âŒ No | Individual assessments |
| **Conservative (diffs only)** | ~3k each | N | $0.0002Ã—N | âŒ No | Quick screening |

**Recommended**: Use **Full Context + Cached** for best accuracy and flexibility!

See [BEST_APPROACH.md](BEST_APPROACH.md) and [TOKEN_USAGE_VISUAL_COMPARISON.md](TOKEN_USAGE_VISUAL_COMPARISON.md) for detailed comparison.


## Six-Dimensional Evaluation Framework

### 1. AI Model Full-Stack & Trade-off Capability (AIæ¨¡å‹å…¨æ ˆä¸æƒè¡¡èƒ½åŠ›)
**Focus Areas:** Research-production mutual promotion innovation, system optimization
- Deep learning framework usage and optimization
- Model selection and trade-off decisions
- End-to-end AI system implementation

### 2. AI Native Architecture & Communication Design (AIåŸç”Ÿæ¶æ„ä¸æ²Ÿé€šè®¾è®¡)
**Focus Areas:** Production-level platform, research-production mutual promotion innovation
- AI-first architecture design
- API and interface design for AI systems
- Documentation and communication patterns

### 3. Cloud Native & Constraint Engineering (äº‘åŸç”Ÿä¸çº¦æŸå·¥ç¨‹åŒ–)
**Focus Areas:** Production-level platform, system optimization
- Containerization and orchestration
- Infrastructure as code
- CI/CD pipeline implementation
- Resource optimization and constraints

### 4. Open Source Collaboration & Requirements Translation (å¼€æºåä½œä¸éœ€æ±‚è½¬åŒ–)
**Focus Areas:** Open source co-construction, research-production mutual promotion innovation
- Open source contribution quality and frequency
- Issue management and PR reviews
- Requirements analysis and implementation

### 5. Intelligent Development & Human-Machine Collaboration (æ™ºèƒ½å¼€å‘ä¸äººæœºååŒ)
**Focus Areas:** All specialties
- AI-assisted development practices
- Code generation and review
- Automation and tooling

### 6. Engineering Leadership & System Trade-offs (å·¥ç¨‹é¢†å¯¼ä¸ç³»ç»Ÿæƒè¡¡)
**Focus Areas:** System optimization, production-level platform, research-production mutual promotion innovation
- Technical decision making
- System architecture trade-offs
- Team collaboration and mentorship

## Features

- **AI-Powered Analysis**: Uses Claude 4.5 Haiku to analyze actual code changes and commit patterns
- **Dual Platform Support**: Analyzes both GitHub and Gitee repositories
- **Smart Caching**: Local storage system to minimize API calls and LLM token usage
- **Modern UI**: React + Antd + Antd-x dashboard with radar charts and detailed breakdowns
- **Real Commit Analysis**: Evaluates actual diffs, file changes, and code quality (not just keywords)

## Input & Output

**Input:**
- Repository URLs (GitHub/Gitee)
- Contributor username

**Output:**
- Six-dimensional scores (0-100 scale)
- AI-generated reasoning and analysis
- Commit statistics and code metrics
- Visual radar chart and detailed breakdowns

## Installation

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Environment Variables

Create a `.env.local` file with your API keys:

```bash
# OpenRouter API key for LLM evaluation
OPEN_ROUTER_KEY=sk-or-v1-your-key-here

# Optional: GitHub token for higher rate limits
GITHUB_TOKEN=ghp_your-token-here

# Optional: Gitee token
GITEE_TOKEN=your-gitee-token-here
```

See `.env.example` for reference.

## Project Structure

```
.
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ README_EVALUATION.md        # Detailed technical documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ server.py                  # FastAPI backend server
â”œâ”€â”€ dashboard.html             # Frontend UI (React + Antd)
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py                # Main evaluation engine
â”‚   â”œâ”€â”€ dimensions.py          # Six dimension evaluators
â”‚   â”œâ”€â”€ commit_evaluator.py    # LLM-powered commit analysis
â”‚   â”œâ”€â”€ collectors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ github.py          # GitHub API integration
â”‚   â”‚   â””â”€â”€ gitee.py           # Gitee API integration
â”‚   â”œâ”€â”€ analyzers/             # (Reserved for future use)
â”‚   â””â”€â”€ reporters/             # (Reserved for future use)
â””â”€â”€ data/                      # Cached commit data
    â””â”€â”€ {owner}/{repo}/
        â”œâ”€â”€ commits/           # Individual commit files
        â””â”€â”€ commits_list.json
```

## How It Works

### 1. Commit Collection
- Fetches commits from GitHub/Gitee API
- Retrieves detailed commit data including files changed and diffs
- Caches data locally in `./data/{owner}/{repo}/commits/` to reduce API calls

### 2. LLM-Powered Analysis
The system sends commit data to Claude 4.5 Haiku with:
- Commit messages and descriptions
- File changes (additions/deletions by language)
- Code diffs (patches)
- Commit statistics and patterns

### 3. Evaluation Criteria

The LLM evaluates based on actual code evidence:

**AI Full-Stack**
- ML framework usage (TensorFlow, PyTorch, etc.)
- Model architecture implementations
- Training and optimization code
- Model deployment patterns

**AI Architecture**
- API design quality
- Service architecture patterns
- Documentation quality
- Integration and interface design

**Cloud Native**
- Docker/Kubernetes configurations
- CI/CD pipeline definitions
- Infrastructure as Code
- Cloud platform integration

**Open Source Collaboration**
- Commit message clarity
- Issue/PR references and linking
- Code review participation
- Refactoring and improvement quality

**Intelligent Development**
- Test coverage and automation
- Development tooling and scripts
- Build configurations
- AI-assisted development practices

**Engineering Leadership**
- Performance optimizations
- Security considerations
- Best practice adoption
- Architectural decision making

### 4. Scoring
- Each dimension scored 0-100
- Scores based on actual code analysis (not keywords)
- LLM provides detailed reasoning for each evaluation
- Caching ensures efficient token usage

## API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/api/evaluate/{owner}/{repo}/{username}` | POST | Evaluate engineer capabilities |
| `/api/cache/stats` | GET | View cache statistics |
| `/api/cache/clear` | DELETE | Clear all cached data |

## Architecture

```
Frontend (dashboard.html - React + Antd)
    â†“
FastAPI Server (server.py)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    â”‚
GitHubCollector/GiteeCollector   CommitEvaluator
(collectors/*.py)                (commit_evaluator.py)
    â†“                                 â†“
GitHub/Gitee API              OpenRouter API
    â†“                                 â†“
Local Cache (data/)           Claude 4.5 Haiku
```

## Limitations & Considerations

- **API Rate Limits**: GitHub API has rate limits (60/hour without token, 5000/hour with token)
- **LLM Costs**: Each evaluation costs ~$0.02-0.05 depending on commit volume
- **Analysis Depth**: Analyzes up to 30 commits per evaluation by default (configurable)
- **Caching**: Smart local caching minimizes API calls and LLM token usage

## Roadmap

- [ ] Complete Gitee integration
- [ ] Batch evaluation of all contributors
- [ ] Historical trend analysis
- [ ] Team-level aggregation
- [ ] Export reports (PDF, JSON)
- [ ] Comparison between contributors
- [ ] Time-series skill evolution
- [ ] Integration with CI/CD pipelines
- [ ] Enhanced UI with more visualizations
- [ ] æˆ‘ä»¬å¯¹äºå­¦é™¢æ¯ä¸€ä¸ªå­¦ç”Ÿå’Œä»–çš„æ¯ä¸€ä¸ªé¡¹ç›®å’Œ PR è®°å½•ï¼Œéƒ½ä¼šä¸‹è½½å¹¶åšåå¤„ç†ã€‚ è¿™ä¸ªéœ€è¦å¤šå°‘é’±ï¼Œ æˆ‘ä»¬éƒ½è¦æ”¯æŒã€‚  ï¼ˆgiteeï¼Œgithubï¼‰ã€‚ æˆ‘ä»¬çš„äº§å“ä¸èƒ½åªæ‰«æä¸€ä¸ªç”¨æˆ·çš„ 10 ä¸ªcommit å°±å¯ä»¥åšå‡ºåˆ¤æ–­ã€‚ ğŸ˜„  æˆ‘ä»¬å¯ä»¥ä¼°ç®—ä¸€ä¸‹ï¼Œ 600 ä¸ªä¸åŒç¨‹åº¦çš„å­¦ç”Ÿåœ¨å„ç§å¼€æºé¡¹ç›®ä¸­ç•™ä¸‹çš„ commitï¼Œåˆå¹¶èµ·æ¥ä¼°è®¡å¤šå°‘ï¼Ÿ å¦‚æœåªæ˜¯æºæ–‡ä»¶å’Œæ–‡å­—ï¼Œ ä¼°è®¡æ˜¯ 100G - 1Tï¼Ÿ 
- [ ] ä¸”å¯ä»¥æ’é™¤ä¸€äº› éå¿…è¦çš„æäº¤æ–‡ä»¶, æ¯”å¦‚/node_modules ç­‰ä¾èµ–æ–‡ä»¶ (å¯ä»¥åŠ å…¥gitignoreä½†æ˜¯æ²¡æœ‰åŠ å…¥çš„, ç­‰ä¸è§„èŒƒè¡Œä¸º)

## License

MIT License

## Documentation

For detailed technical documentation, see [README_EVALUATION.md](README_EVALUATION.md)

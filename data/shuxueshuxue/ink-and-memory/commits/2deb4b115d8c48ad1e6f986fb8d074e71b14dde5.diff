*** FILE: backend/server.py ***
@@ -10,12 +10,13 @@
 
 import asyncio
 import httpx
-from fastapi import FastAPI, HTTPException, Depends, Header
+from fastapi import FastAPI, HTTPException, Depends, Header, WebSocket
 from fastapi.middleware.cors import CORSMiddleware
 from polycli.orchestration.session_registry import session_def, get_registry
 from polycli.integrations.fastapi import mount_control_panel
 from polycli import PolyAgent
 from stateless_analyzer import analyze_stateless
+from speech_recognition import init_speech_recognition
 import config
 from proxy_config import get_image_api_proxies
 from typing import Optional
@@ -1623,6 +1624,13 @@ def get_friend_timeline(friend_id: int, limit: int = 30, current_user: dict = De
         raise HTTPException(status_code=403, detail="Not friends or friend not found")
     return {"pictures": timeline}
 
+@app.websocket("/ws/speech-recognition")
+async def speech_recognition(websocket: WebSocket):
+    # TODO: find a way of authentication for websocket
+    await websocket.accept()
+    await init_speech_recognition(websocket)
+
+
 # @@@ Removed /api/analyze wrapper - frontend now calls /polycli/api/trigger-sync directly
 
 # @@@ Removed /api/chat wrapper - frontend now calls /polycli/api/trigger-sync directly

*** FILE: backend/speech_recognition.py ***
@@ -0,0 +1,72 @@
+from fastapi import FastAPI, WebSocket, WebSocketDisconnect
+from dashscope.audio.asr import Recognition, RecognitionCallback, RecognitionResult
+import asyncio
+import json
+from typing import cast
+
+import dashscope
+# TODO: move api key to config
+dashscope.api_key = "sk-c4063b5a8e094b1691875f05b700a036"
+
+SAMPLE_RATE = 16000
+MODEL_NAME = "paraformer-realtime-v2"
+
+app = FastAPI()
+
+async def init_speech_recognition(websocket: WebSocket):
+    callback = Callback(websocket)
+    recognition = Recognition(
+        model=MODEL_NAME,
+        format="pcm",
+        sample_rate=SAMPLE_RATE,
+        callback=callback,
+    )
+    recognition.start()
+    print("Recognition started (model=%s, sample_rate=%d)" % (MODEL_NAME, SAMPLE_RATE))
+    try:
+        while True:
+            try:
+                data = await websocket.receive_bytes()
+                recognition.send_audio_frame(data)
+            except Exception as e:
+                print("Error during speech streaming: ", e)
+                break
+    except WebSocketDisconnect:
+        recognition.stop()
+    except Exception as e:
+        print("WebSocket handler error:", e)
+
+class Callback(RecognitionCallback):
+    def __init__(self, websocket: WebSocket) -> None:
+        self.websocket = websocket
+
+    def on_event(self, result: RecognitionResult) -> None:
+        """
+        Called by the recognition library when there's an update.
+        Compute incremental new_word and schedule broadcast to connected clients.
+
+        Example response from model:
+        {
+            'sentence_id': 1,
+            'begin_time': 0,
+            'end_time': None,
+            'text': '你好世界',
+            'channel_id': 0,
+            'speaker_id': None,
+            'sentence_end': False,
+            'words': [
+                {'begin_time': 0, 'end_time': 940, 'text': '你好', 'punctuation': '', 'fixed': False, 'speaker_id': None},
+                {'begin_time': 940, 'end_time': 1880, 'text': '世界', 'punctuation': '', 'fixed': False, 'speaker_id': None},
+            ]
+        }
+        """
+        sentence = result.get_sentence()
+        sentence = cast(dict, sentence)
+
+        sentence_id = sentence.get("sentence_id") or -1
+        text = sentence.get("text")
+        res = json.dumps({"id": sentence_id, "sentence": text})
+
+        async def send_text():
+            await self.websocket.send_text(res)
+        asyncio.run(send_text())
